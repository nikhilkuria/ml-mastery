{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages\n",
      "Requirement already up-to-date: tensorboard<1.9.0,>=1.8.0 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already up-to-date: wheel>=0.26 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already up-to-date: absl-py>=0.1.6 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already up-to-date: astor>=0.6.0 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting protobuf>=3.4.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/6d/7e/51c91b28cb8446ebd7231d375a2025bca4c59d15ddc0cf2dd0867b400cd7/protobuf-3.6.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already up-to-date: termcolor>=1.1.0 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/57/da122cbfc1b7815381480b23044fff06b90f58c1be9310e68c2d6b1d623c/grpcio-1.12.1-cp36-cp36m-macosx_10_7_intel.whl (1.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.9MB 691kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: gast>=0.2.0 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already up-to-date: six>=1.10.0 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting numpy>=1.13.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/cd/b2c50b5190b66c711c23ef23c41d450297eb5a54d2033f8dcb3b8b13ac85/numpy-1.14.5-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.7MB 301kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: markdown>=2.6.8 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "Requirement already up-to-date: bleach==1.5.0 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "Requirement already up-to-date: werkzeug>=0.11.10 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "Requirement already up-to-date: html5lib==0.9999999 in /Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "Collecting setuptools (from protobuf>=3.4.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/7f/e1/820d941153923aac1d49d7fc37e17b6e73bfbd2904959fffbad77900cf92/setuptools-39.2.0-py2.py3-none-any.whl\n",
      "Installing collected packages: setuptools, protobuf, grpcio, numpy\n",
      "  Found existing installation: setuptools 28.8.0\n",
      "    Uninstalling setuptools-28.8.0:\n",
      "      Successfully uninstalled setuptools-28.8.0\n",
      "  Found existing installation: protobuf 3.5.2.post1\n",
      "    Uninstalling protobuf-3.5.2.post1:\n",
      "      Successfully uninstalled protobuf-3.5.2.post1\n",
      "  Found existing installation: grpcio 1.12.0\n",
      "    Uninstalling grpcio-1.12.0:\n",
      "      Successfully uninstalled grpcio-1.12.0\n",
      "  Found existing installation: numpy 1.14.3\n",
      "    Uninstalling numpy-1.14.3:\n",
      "      Successfully uninstalled numpy-1.14.3\n",
      "Successfully installed grpcio-1.12.1 numpy-1.14.5 protobuf-3.6.0 setuptools-39.2.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install tensorflow\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nkose/dev/repo/ml-mastery/_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "# Configure imports and enable eager execution\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what is eager execution\n",
    "https://www.tensorflow.org/programmers_guide/eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris classification problem\n",
    "\n",
    "Imagine you are a botanist seeking an automated way to categorize each Iris flower you find. Machine learning provides many algorithms to statistically classify flowers. For instance, a sophisticated machine learning program could classify flowers based on photographs. Our ambitions are more modest—we're going to classify Iris flowers based on the length and width measurements of their [sepals](https://en.wikipedia.org/wiki/Sepal) and [petals](https://en.wikipedia.org/wiki/Petal).\n",
    "\n",
    "The Iris genus entails about 300 species, but our program will classify only the following three:\n",
    "\n",
    "* Iris setosa\n",
    "* Iris virginica\n",
    "* Iris versicolor\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.tensorflow.org/images/iris_three_species.jpg\"\n",
    "         alt=\"Petal geometry compared for three iris species: Iris setosa, Iris virginica, and Iris versicolor\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://commons.wikimedia.org/w/index.php?curid=170298\">Iris setosa</a> (by <a href=\"https://commons.wikimedia.org/wiki/User:Radomil\">Radomil</a>, CC BY-SA 3.0), <a href=\"https://commons.wikimedia.org/w/index.php?curid=248095\">Iris versicolor</a>, (by <a href=\"https://commons.wikimedia.org/wiki/User:Dlanglois\">Dlanglois</a>, CC BY-SA 3.0), and <a href=\"https://www.flickr.com/photos/33397993@N05/3352169862\">Iris virginica</a> (by <a href=\"https://www.flickr.com/photos/33397993@N05\">Frank Mayfield</a>, CC BY-SA 2.0).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fortunately, someone has already created a [data set of 120 Iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set) with the sepal and petal measurements. This is a classic dataset that is popular for beginner machine learning classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and parse the training dataset\n",
    "\n",
    "We need to download the dataset file and convert it to a structure that can be used by this Python program.\n",
    "\n",
    "### Download the dataset\n",
    "\n",
    "Download the training dataset file using the [tf.keras.utils.get_file](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) function. This returns the file path of the downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://download.tensorflow.org/data/iris_training.csv\n",
      "8192/2194 [================================================================================================================] - 0s 0us/step\n",
      "Local copy of the dataset file: /Users/nkose/.keras/datasets/iris_training.csv\n"
     ]
    }
   ],
   "source": [
    "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "\n",
    "This dataset, `iris_training.csv`, is a plain text file that stores tabular data formatted as comma-separated values (CSV). Use the `head -n5` command to take a peak at the first five entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120,4,setosa,versicolor,virginica\r\n",
      "6.4,2.8,5.6,2.2,2\r\n",
      "5.0,2.3,3.3,1.0,1\r\n",
      "4.9,2.5,4.5,1.7,2\r\n",
      "4.9,3.1,1.5,0.1,0\r\n"
     ]
    }
   ],
   "source": [
    "!head -n5 {train_dataset_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the dataset\n",
    "\n",
    "Since our dataset is a CSV-formatted text file, we'll parse the feature and label values into a format our Python model can use. Each line—or row—in the file is passed to the `parse_csv` function which grabs the first four feature fields and combines them into a single tensor. Then, the last field is parsed as the label. The function returns *both* the `features` and `label` tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(line):\n",
    "  example_defaults = [[0.], [0.], [0.], [0.], [0]]  # sets field types\n",
    "  parsed_line = tf.decode_csv(line, example_defaults)\n",
    "  # print(\"parsed line: \", parsed_line)\n",
    "  # First 4 fields are features, combine into single tensor\n",
    "  features = tf.reshape(parsed_line[:-1], shape=(4,))\n",
    "  # print(\"features : \", features)\n",
    "  # Last field is the label\n",
    "  label = tf.reshape(parsed_line[-1], shape=())\n",
    "  # print(\"label : \", label)\n",
    "  return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed line:  [<tf.Tensor: id=24, shape=(), dtype=float32, numpy=6.4>, <tf.Tensor: id=25, shape=(), dtype=float32, numpy=2.8>, <tf.Tensor: id=26, shape=(), dtype=float32, numpy=5.6>, <tf.Tensor: id=27, shape=(), dtype=float32, numpy=2.2>, <tf.Tensor: id=28, shape=(), dtype=int32, numpy=2>]\n",
      "features :  tf.Tensor([6.4 2.8 5.6 2.2], shape=(4,), dtype=float32)\n",
      "label :  tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=36, shape=(4,), dtype=float32, numpy=array([6.4, 2.8, 5.6, 2.2], dtype=float32)>,\n",
       " <tf.Tensor: id=39, shape=(), dtype=int32, numpy=2>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_csv(\"6.4,2.8,5.6,2.2,2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is `tf.Tensor`?\n",
    "\n",
    "Represents one of the outputs of an Operation.\n",
    "\n",
    "A Tensor is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation's output, but instead provides a means of computing those values in a TensorFlow tf.Session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training tf.data.Dataset\n",
    "\n",
    "TensorFlow's [Dataset API](https://www.tensorflow.org/programmers_guide/datasets) handles many common cases for feeding data into a model. This is a high-level API for reading data and transforming it into a form used for training. See the [Datasets Quick Start guide](https://www.tensorflow.org/get_started/datasets_quickstart) for more information.\n",
    "\n",
    "This program uses [tf.data.TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) to load a CSV-formatted text file and is parsed with our `parse_csv` function. A [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) represents an input pipeline as a collection of elements and a series of transformations that act on those elements. Transformation methods are chained together or called sequentially—just make sure to keep a reference to the returned `Dataset` object.\n",
    "\n",
    "Training works best if the examples are in random order. Use `tf.data.Dataset.shuffle` to randomize entries, setting  `buffer_size` to a value larger than the number of examples (120 in this case). To train the model faster, the dataset's *[batch size](https://developers.google.com/machine-learning/glossary/#batch_size)* is set to `32` examples to train at once.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example features: tf.Tensor([5.1 3.7 1.5 0.4], shape=(4,), dtype=float32)\n",
      "example label: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TextLineDataset(train_dataset_fp)\n",
    "train_dataset = train_dataset.skip(1)             # skip the first header row\n",
    "train_dataset = train_dataset.map(parse_csv)      # parse each row\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000)  # randomize\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "# View a single example entry from a batch\n",
    "features, label = iter(train_dataset).next()\n",
    "print(\"example features:\", features[0])\n",
    "print(\"example label:\", label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model using Keras\n",
    "\n",
    "The TensorFlow [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras) API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together. See the [Keras documentation](https://keras.io/) for details.\n",
    "\n",
    "The [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, two [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers with 10 nodes each, and an output layer with 3 nodes representing our label predictions. The first layer's `input_shape` parameter corresponds to the amount of features from the dataset, and is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=\"relu\", input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *[activation function](https://developers.google.com/machine-learning/crash-course/glossary#activation_function)* determines the output of a single neuron to the next layer. This is loosely based on how brain neurons are connected. There are many [available activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations), but [ReLU](https://developers.google.com/machine-learning/crash-course/glossary#ReLU) is common for hidden layers.\n",
    "\n",
    "The ideal number of hidden layers and neurons depends on the problem and the dataset. Like many aspects of machine learning, picking the best shape of the neural network requires a mixture of knowledge and experimentation. As a rule of thumb, increasing the number of hidden layers and neurons typically creates a more powerful model, which requires more data to train effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss and gradient function\n",
    "\n",
    "Both training and evaluation stages need to calculate the model's *[loss](https://developers.google.com/machine-learning/crash-course/glossary#loss)*. This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. We want to minimize, or optimize, this value.\n",
    "\n",
    "Our model will calculate its loss using the [tf.losses.sparse_softmax_cross_entropy](https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy) function which takes the model's prediction and the desired label. The returned loss value is progressively larger as the prediction gets worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is the expected resutlt\n",
    "# y is the actual result\n",
    "def loss(model, x, y):  \n",
    "  y_ = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` function uses the `loss` function and the [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) to record operations that compute the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize our model. For more examples of this, see the [eager execution guide](https://www.tensorflow.org/programmers_guide/eager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
